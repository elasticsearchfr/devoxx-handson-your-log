---
layout: default
---

<div class="page-header">
	<h1>{{ site.name }}</h1>
	<p class="lead">Logstash + Elasticsearch + Kibana</p>
</div>

<h1>Pré-requis</h1>
<ul>
	<li>une archive contenant kibana : <a href="{{ site.kibana_url }}">{{ site.kibana_url_title }}</a></li>
    <li>un jar logstash : <a href="{{ site.logstash_url }}">{{site.logstash_url_title}}</a></li>
    <li>un jar contenant un générateur de log : <a href="{{ site.loggenerator_url }}">{{ site.loggenerator_url_title }}</a></li>
    <li>une archive contenant elasticsearch : <a href="{{ site.elasticsearch_url }}">{{ site.elasticsearch_url_title }}</a></li>
    <li>un fichier contenant des données de géolocalisation : <a href="{{ site.geolitecity_url }}">{{ site.geolitecity_url_title }}</a></li>
</ul>

<h1>Logstash</h1>
<p>
	Logstash est un pipe permettant de collecter, parser, et stocker des logs à l'aide d'entrées, de filtres et de sorties (input, filter, output).
	La phase de parsing permet d'ajouter de la sémantique à notre événement, en ajoutant, modifiant ou supprimant des champs, des tags, des types, etc...
</p>
<p>
	Dans cette première partie de l'atelier, nous allons donc découvrir Logstash et le configurer pour structurer nos logs afin qu'ils soient facilement exploitables par la suite.
</p>

<h2>Découverte de Logstash</h2>
<p>
	Commencer tout d'abord par créer le répertoire <code>~/workshop</code>.
	Copier le jar Logstash disponible dans le répertoire <code>~/tools</code> dans le répertoire <code>~/workshop</code>.
	Dans le répertoire ~/workshop, créer un fichier de configuration Logstash nommé <code>logstash-logback.conf</code>.
</p>

<pre>
input {
  stdin { } 
}

output {
  stdout { debug => true }
}
</pre>

<p>Vous pouvez maintenant exécuter Logstash grâce à la commande suivante:</p>

<pre>
$ java -jar logstash-1.2.1-flatjar.jar agent -f logstash-logback.conf
</pre>

<p>Logstash est prêt à interpréter ce qu'il recevera sur l'entrée standard. Pour un premier test, passez lui la ligne suivante en entrée:</p>

<pre>02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT</pre>

<p>Vous devez normalement voir un message de la forme suivante s'afficher:</p>

<pre>
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-25T07:50:26.232Z",
    "@version" => "1",
    "host" => "lucid64"
}
</pre>

<p>
	On constate que le timestamp enregistré n'est pas du tout lié à celui de la ligne de log. 
	Logstash n'a pas pris en compte la date incluse dans notre log, 
	il va falloir configurer entre autres un filtre pour cela.
</p>

<h2>Ajouter de la sémantique</h2>
<p>
	Pour configurer Logstash afin qu'il puisse interpréter les données qu'il va recevoir en entrée, 
	nous allons configurer plusieurs filtres.
	Logstash execute le fichier de configuration en prenant en compte l'ordre de déclaration de vos filtres.
</p>

<h3>Filtre Grok</h3>
<p>
	Le filtre Grok met à votre disposition plusieurs patterns pour parser les lignes de logs.
	Ressources: 
</p>
<ul>
	<li><a href="http://logstash.net/docs/{{site.logstash_version}}/filters/grok">Documentation du filtre Grok</a></li>
	<li><a href="https://github.com/logstash/logstash/blob/master/patterns/grok-patterns">Les patterns grok pré-définis</a></li>
	<li><a href="http://grokdebug.herokuapp.com/">Debugger</a></li>
</ul>
<p>
	Dans un premier temps, nous voulons juste parser le niveau de log. Pour celà, le pattern <code>LOGLEVEL</code> va nous être utile.
</p>

<pre>
filter {
   grok {
      match => ["message","%{LOGLEVEL:log_level}"]
   }
}
</pre>

<p>
	Vous pouvez relancer Logstash et repasser la ligne de log:
</p>

<pre>02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT</pre>

<p>Cette fois, on remarque qu'un élément a été analysé, l'élément <code>log_level</code></p>

<pre>
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-27T08:46:51.132Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_level" => "INFO"
}
</pre>

<p>
	Nous allons enrichir notre filtre pour parser le reste des lignes:
</p>

<pre>
filter {
   grok {
      match => ["message","(?<log_date>%{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}.[0-9]{3}) \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
   }
}
</pre>

<p>
	De nouveau, nous pouvons relancer Logstash et lui passer notre ligne de log, cette fois, nous allons avoir un résultat de la forme suivante:
</p>

<pre>
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-27T10:30:32.242Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "02-10-2013 14:26:27.724",
    "thread" => "pool-10-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=9205,ip=217.109.49.180,cat=TSHIRT"
}
</pre>

<p>
	Pour alléger la configuration, nous allons extraire l'expression régulière de la date et la définir en tant que pattern :
</p>

<ul>
	<li>créez un dossier "patterns" dans le répertoire courant</li>
	<li>créez un fichier "logback" dans le dossier "patterns" contenant:</li>
</ul>

<pre>LOG_DATE %{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}.[0-9]{3}</pre>

<p>
	Indiquez ensuite à Grok le dossier contenant vos fichiers de patterns via l'attribut <code>patterns_dir</code>.
    <br />
    La configuration Losgstash devient :
</p>

<pre>
filter { 
   grok {
      patterns_dir => "./patterns"
      match => [ "message", "%{LOG_DATE:log_date} \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
   }
}
</pre>

<p>
	À ce niveau, nous constatons que l'intégralité du message a été parsé et est maintenant interprété. 
	Par contre, même si nous avons correctement interprété la date du log, 
	le champ <code>@timestamp</code> contient toujours la date de lecture par Logstash. 
	Il serait plus intéressant de mettre dans ce champ la date de log. 
	Pour cela, il va falloir utiliser un autre type de filtre, le filtre date.
</p>

<h3>Filtre date</h3>
<p>
	<a href="http://logstash.net/docs/{{site.logstash_version}}/filters/date">Documentation du filtre date</a>
</p>
<p>
	Le filtre date est l'un des filtres les plus important. 
	Il permet de préciser quelle date utiliser pour l'événement généré et alimentant le champ <code>@timestamp</code>. 
	Rajoutez le filtre dans le fichier de configuration: 
</p>

<pre>
filter {
    [...]
    date {
        match => ["log_date","dd-MM-YYYY HH:mm:ss.SSS"]
    }
}
</pre>

<p>
	En redonnant notre ligne de log en entrée, nous récupérons un retour de la forme suivante :
</p>

<pre>
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-02T12:26:27.724Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "02-10-2013 14:26:27.724",
    "thread" => "pool-10-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=9205,ip=217.109.49.180,cat=TSHIRT"
}
</pre>

<p>
	Logstash normalise les dates au format UTC automatiquement.
</p>

<h3>Filtre kv</h3>
<p>
	Le filtre kv s'avère très utile lorsque vous voulez parser un champ de type <code>foo=bar</code> comme par exemple une requête HTTP. 
	Ajoutez le filtre kv pour notre example : 
</p>

<pre>
filter {
    [...]
    kv {
      field_split => ","
      source => "msg"
    }
}
</pre>

<p>
	Relancez Logstash et passez lui en entrée la ligne suivante : 
</p>

<pre>08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0</pre>

<p>
	Vous devez obtenir le résultat suivant :
</p>

<pre>
{
    "message" => "08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0",
    "@timestamp" => "2013-10-08T14:33:49.629Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "08-10-2013 16:33:49.629",
    "thread" => "pool-1-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0",
    "id" => "41",
    "ip" => "157.55.34.94",
    "brand" => "Apple",
    "name" => "iPhone 5C",
    "model" => "iPhone 5C - Blanc - Disque 16Go",
    "category" => "Mobile",
    "color" => "Blanc",
    "options" => "Disque 16Go",
    "price" => "599.0"
}
</pre> 

<p>
	Logstash parse maintenant notre ligne de vente et ajoute automatiquement les champs 
	<code>category</code>, <code>brand</code>, <code>name</code>, <code>model</code>, 
	<code>color</code>, <code>options</code> et <code>price</code>.
</p>

<h3>Filtre GeoIP</h3>
<p>
	Le filtre geoip permet d'ajouter des informations de géolocalisation via une adresse ip (ou hostname). 
	Logstash utilise la base de donnée <a href="https://www.maxmind.com/app/geolite">GeoCityLite de Maxmind</a> sous license CCA-ShareAlike 3.0.
</p>

<p>
	Nous allons utiliser une version de GeoCityLite téléchargée au préalable sur le site Maxmind plutôt que la version embarquée dans Logstash.
	Copiez le fichier <code>~/tools/GeoLiteCity.dat</code> dans <code>~/workshop</code> et rajoutez le filtre dans la configuration Logstash :
</p>

<pre>
filter {
    [...]

    geoip {
        source => "ip"
        database => "./GeoLiteCity.dat"
    }
}
</pre>

<h3>Filtre mutate</h3>
<p>
	Le filtre mutate est un filtre "couteaux suisses" permettant une multitude de modifications.
</p>

<h4>Ajout de tag</h4>
<p>Nous allons ajouter un tag à nos logs afin de différencier les recherches des ventes :</p>

<pre>
filter {
    [...]
    if [classname] =~ /SellRequest/ {
        mutate { add_tag => "sell" }
    } else if [classname] =~ /SearchRequest$/ {
        mutate { add_tag => "search" }
    }
}
</pre>

<p>
	Relancez Logstash et passez lui en entrée la ligne suivante :
</p>

<pre>
08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0
08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SellRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0
</pre>

<p>
	Notez au passage que Logstash permet l'utilisation de conditions, pour en savoir plus : 
	<a href="http://logstash.net/docs/{{site.logstash_version}}/configuration#conditionals">Documentation</a>
</p>

<h4>Conversion de type</h4>
<p>
	Le filtre mutate permet de convertir certains champs en entier, flottant ou string. 
	Nous ajoutons à notre configuration la conversion des champs id et price :
</p>

<pre>
filter {
    [...]
    mutate {
        convert => [ "id", "integer" ]
        convert => [ "price", "float" ]
    }   
}
</pre>

<h4>Suppression d'un champ</h4>
<p>
	Toujours avec le filtre <code>mutate</code>, nous allons supprimer le champ <code>msg</code>. 
	Nous avons en effet parsé ce champ avec le filtre <code>kv</code> et n'avons plus besoin de ce doublon d'information. 
</p>

<pre>
filter {
    [...] 
    mutate {
        [...]
        remove_field => [ "msg" ]  
    }  
}
</pre>

<h4>Split d'un champ</h4>
<p>
	Pour finir avec le filtre <code>mutate</code>, 
	nous allons splité notre champ <code>options</code> afin d'avoir un tableau d'options.
</p>

<pre>
filter {
    [...]
    mutate {
        [...]
       split => [ "options", "|" ]
    }
}
</pre>

<h3>GeoIP et Bettermap</h3>
<p>
Le panel Bettermap de Kibana requiert un champ contenant les coordonnées GPS au format Geo_JSON 
(i.e: un tableau de deux float au format: [ longitude, latitude ]).

Ajoutez un champ <code>geoip.lnglat</code> contenant le tableau de coordonnées via le "hack" suivant: 
</p>

<pre>
filter {
    [...]
    # geoip.lnglat contiendra le point geo_json, 'tmplat' contient la latitude (temporaire)
    # les deux champs sont de type string.
    mutate {
        add_field => [ "[geoip][lnglat]", "%{[geoip][longitude]}", "tmplat", "%{[geoip][latitude]}" ]
    }

    # merge du champ tmplat dans geoip.lnglat. 
    # le champ geoip.lnglat devient un tableau de string
    mutate {
        merge => [ "[geoip][lnglat]", "tmplat" ]
    }

    # conversion du tableau de string en float
    # suppression du champ tmplat
    mutate {
        convert => [ "[geoip][lnglat]", "float" ]
        remove_field => [ "tmplat" ]
    }
}
</pre>

<h3>Résultat final</h3>

<pre>
input {
  stdin { } 
}

filter {
    grok {
        patterns_dir => "./patterns"
        match => ["message","%{LOG_DATE:log_date} \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
    }

    date {
        #timezone => "UTC"
        match => ["log_date","dd-MM-YYYY HH:mm:ss.SSS"]
    }

    kv {
        field_split => ","
        source => "msg"
    }

    geoip {
        source => "ip"
        database => "./GeoLiteCity.dat"
    }

    if [classname] =~ /SellRequest/ {
        mutate { add_tag => "sell" }
    } else if [classname] =~ /SearchRequest$/ {
        mutate { add_tag => "search" }
    }

    mutate {
        convert => [ "id", "integer" ]
        convert => [ "price", "float" ]
        remove_field => [ "msg" ]
        split => [ "options", "|" ]
    }

    # hack pour Bettermap panel de Kibana
    mutate {
        add_field => [ "[geoip][lnglat]", "%{[geoip][longitude]}", "tmplat", "%{[geoip][latitude]}" ]
    }

    mutate {
        merge => [ "[geoip][lnglat]", "tmplat" ]
    }

    mutate {
        convert => [ "[geoip][lnglat]", "float" ]
        remove_field => [ "tmplat" ]
    }

}

output {
    stdout { debug => true }
}
</pre>

<h3>Lancer la génération de log</h3>
<p>
	Maintenant que Logstash est capable d'analyser nos logs, nous allons lancer notre générateur.
</p>

<h3>Création du dossier contenant les futurs logs</h3>
<pre>$ mkdir /tmp/logstash</pre>

<h3>Modifier l'entrée de Logstash</h3>
<p>
	Afin que Logstash analyse tous les fichiers de log contenu dans le dossier précédemment créé.
</p>

<pre>
input {
    file {
        path => "/tmp/logstash/*.log"
    }
}
</pre>

<h3>Lancez le générateur de log</h3>
<ul>
	<li>Copiez le jar <code>~/tools/log-generator.jar</code> dans le dossier <code>~/workshop</code></li>
    <li>Lancer le générateur afin de générer 10 lignes de log chaque seconde dans le fichier <code>/tmp/logstash/workshop.log</code></li>
</ul>
<pre>$ java -jar log-generator.jar -n 10 -r 1000 > /tmp/logstash/workshop.log &amp;</pre>

<h1>ElasticSearch</h1>
<p>
	Maintenant que Logstash est configuré pour parser nos logs et les transformer dans un format convenable, 
	nous allons stocker ces logs dans ElasticSearch.
</p>

<h2>Configuration d'ElasticSearch</h2>
<p>
	Décompressez l'archive elasticsearch :
</p>
<pre>tar xvzf {{site.elasticsearch_url_title}}</pre>

<h2>Template de mapping ElasticSearch pour les index Logstash</h2>
<p>
	Lancez ElasticSearch après avoir installé le plugin head :
</p>

<pre>
$ ~/workshop/elasticsearch-0.90.5/bin/plugin -install mobz/elasticsearch-head
$ ~/workshop/elasticsearch-0.90.5/bin/ElasticSearch
</pre> 

<p>
	Une fois elasticsearch lancé, vous pouvez visualiser facilement votre cluster Elasticsearch via l'url: 
	http://io-1-kibana.aws.xebiatechevent.info:9200/_plugin/head
</p>

<p>
	Ajoutez le template suivante afin d'utiliser l'analyser <code>keyword</code> pour les champs 
	<code>ip</code>, <code>name</code>, <code>model</code>, <code>options</code> et <code>email</code> : 
</p>

<pre>
curl -XPUT http://localhost:9200/_template/logstash_per_index -d '{
    "template" : "logstash*",
    "mappings" : {
        "_default_" : {
           "_all" : {"enabled" : false},
           "properties" : {
              "@timestamp": { "type": "date", "index": "not_analyzed" },
              "tags": { "type": "string", "index": "not_analyzed" },
              "ip": { "type" : "ip", "analyzer": "keyword", "index": "analyzed" },
              "name": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "model": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "options": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "email": { "type" : "string", "analyzer": "keyword", "index": "analyzed" }
 
            }
        }
   }
}
'
</pre>

<p>
	Ce mapping vous permettra de ne pas avoir décueils lors de la construction de votre dashboard Kibana.
</p>

<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-analyzed-terms.png" />
</p>

<h2>Branchement de Logstash avec ElasticSearch</h2>
<p>
	ElasticSearch est maintenant configuré. 
	Nous allons donc configurer Logstash pour qu'il envoit les logs analysés dans le moteur de recherche.
	Modification de la sortie de Logstash :
</p>

<pre>
output {
    elasticsearch {
        host => "io-1-kibana.aws.xebiatechevent.info"
    }
}
</pre>

<p>
	C'est tout ce qu'il y a à faire. 
	Vous pouvez maintenant redémarrer Logstash et pour vérifier qu'ElasticSearch est bien alimenté, 
	retourner sur la vm d'ElasticSearch et lancez les deux commandes suivantes.
</p>
<p>
	Pour lister les index :
</p>

<pre>$ curl -s http://127.0.0.1:9200/_status?pretty=true | grep logstash</pre>

<p>
	Pour voir le nombre de documents indexés :
</p>

<pre>$ curl -gs -XGET "http://localhost:9200/logstash-*/_count"</pre>

<h1>Kibana</h1>

<h2>Installation</h2>  
<p>
	Installation de Kibana dans Apache :
</p>

<pre>
cd /var/www
sudo unzip /home/ubuntu/tools/kibana-latest.zip 
sudo mv kibana-latest kibana
sudo /etc/init.d/apache2 restart
</pre>

<p>
	Vous devriez pouvoir acceder au dashboard via l'url: io-1-kibana.aws.xebiatechevent.info/kibana
</p>

<h2>Dashboard Logstash</h2>
<p>
	Kibana vous propose un dashboard pré-configuré si vos données viennent de Logstash/ES. 
	Vous pouvez y acceder via l'url: io-1-kibana.aws.xebiatechevent.info/kibana/index.html#/dashboard/file/logstash.json.
</p>

<h2>Dashboard GeekShop</h2>
<p>
	Nous allons créer un dashboard personnalisé pour notre boutique en ligne. 
	Commencez avec un dashboard vide accessible à l'adresse : 
	io-1-kibana.aws.xebiatechevent.info/kibana/index.html#/dashboard/file/blank.json
	N'oubliez pas de sauvegarder régulièrement votre dashboard dans Elasticsearch grâce au menu situé en haut à droite.
</p>

<h3>Configuration générale</h3>
<p>
	Cliquez sur la roue crantée tout en haut à droite afin de faire apparaître le menu <code>Dashboard Settings</code> :
</p>
<ul>
	<li>dans l'onglet "General", changez le titre du dashboard</li>
    <li>dans l'onglet "Index", selectionnez "day", puis entrez "[logstash-]YYYY.MM.DD" dans le champ index pattern</li>
    <li>dans l'onglet "Rows", ajouter les lignes "timeline", "hits", "search", "sell", "events" en cliquant sur "Create Row"</li>
</ul>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-index-settings.png" />
</p>

<h3>Configuration des queries</h3>
<p>
	Ajoutez des barres de recherches en cliquant sur l'icône "+" 
	<img src="{{site.baseurl}}/assets/images/dashboard-query-buttons.png" />
</p>
<p>
	Entrez les queries suivantes :
</p>
<ul>
	<li>tags: "search"</li>
    <li>tags: "sell"</li>
    <li>sex: "M"</li>
    <li>sex: "F"</li>
</ul>
<p>
	Donnez un alias aux queries (en cliquant sur le rond de couleur) :
</p>
<ul>
	<li>search</li>
	<li>sell</li>
	<li>men</li>
	<li>women</li>
</ul>
<p>
	Puis épinglez les queries via le bouton "Pin".
</p>
<p>
	Pour finir, choisissez un période de temps pour le dashboard en cliquant sur "set at time filter". 
	Sélectionnez "Last 1h" puis "Auto-Refresh" &gt; "every 30s".
</p>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-queries.png" />
</p>

<h3>Ligne Timeline</h3>
<ul>
	<li>
		Ajoutez un panel "histogram" dans la première ligne. 
		Configurez la taille à 6.
	</li>
	<li>
		Ajoutez un panel "bettermap" toujours dans la première ligne. 
		Configurez la taille à 6. Insérez "geoip.lnglat" pour le champ "Coordinate Field".
	</li>
</ul>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-timeline.png" />
</p>

<h3>Ligne Hits</h3>
<ul>
	<li>
		Ajoutez un panel "pie" de taille 2. 
		Selectionnez le mode "goal". 
		Sélectionnez la query "search" (queries => selected).
	</li>
	<li>
    	Ajoutez un panel "pie" de taille 2. 
    	Selectionnez le mode "goal". 
    	Sélectionnez la query "sell".
    </li>
    <li>
    	Ajoutez un panel "hits" de taille 2. 
    	Selectionnez le style "pie". 
    	Sélectionnez les queries "search" et "sell".
    </li>
    <li>
    	Ajoutez un panel "hits" de taille 2. 
    	Selectionnez le style "pie". 
    	Sélectionnez les queries "men" et "women".
    </li>
    <li>
    	Ajoutez un panel "hits" de taille 4. 
    	Selectionnez le style "bar". 
    	Sélectionnez les queries "search", "sell", "men" et "women".
    </li>
</ul>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-hits.png" />
</p>

<h3>Ligne Search</h3>
<p>
	(Précision: veillez à décocher "missing" et "others" dans la configuration de tous les panels suivants)
</p>
<ul>
    <li>
    	Ajoutez un panel "terms" de taille 2. 
    	Insérez "category" dans le champ "Field". 
    	Selectionnez le style "pie". 
    	Sélectionnez la query "search".
    </li>
    <li>
    	Ajoutez un panel "terms" de taille 3. 
    	Insérez "options" dans le champ "Field". 
    	Selectionnez le style "table". 
    	Sélectionnez la query "search".
    </li>
    <li>
    	Ajoutez un panel "terms" de taille 3. 
    	Insérez "geoip.ip" dans le champ "Field". 
    	Selectionnez le style "table". 
    	Sélectionnez la query "search".
    </li>
    <li>
    	Ajoutez un panel "terms" de taille 4. 
    	Insérez "name" dans le champ "Field". 
    	Selectionnez le style "bar". 
    	Sélectionnez la query "search".
    </li>
</ul>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-search.png" />
</p>

<h3>Ligne Sell</h3>
<p>
	(Précision: veillez à décocher "missing" et "others" dans la configuration de tous les panels suivants)
</p>
<ul>
	<li>
		Ajoutez un panel "terms" de taille 2. 
		Insérez "category" dans le champ "Field". 
		Selectionnez le style "pie". 
		Sélectionnez la query "sell".
	</li>
    <li>
    	Ajoutez un panel "terms" de taille 3. 
    	Insérez "email" dans le champ "Field". 
    	Selectionnez le style "table". 
    	Sélectionnez la query "sell".
    </li>
    <li>
    	Ajoutez un panel "terms" de taille 3. 
    	Insérez "name" dans le champ "Field". 
    	Selectionnez le style "table". 
    	Sélectionnez la query "sell".
    </li>
    <li>
    	Ajoutez un panel "terms" de taille 4. 
    	Insérez "model" dans le champ "Field". 
    	Selectionnez le style "bar". 
    	Sélectionnez la query "sell".
    </li>
</ul>
<p>
	<img src="{{site.baseurl}}/assets/images/dashboard-sell.png" />
</p>

<h3>Ligne Events</h3>
<p>
	Ajoutez un panel de type "table" de taille 2. Sélectionnez la query "courante" "*"
</p>

<h2>Conclusion</h2>
<p>
	Vous devez obtenir un <a href="{{site.baseurl}}/assets/dashboard.json">dashboard</a> similaire à celui-ci :
</p>
<p>
	<img src="{{site.baseurl}}/assets/images/geekshop-kibana-part1.png" />
	<img src="{{site.baseurl}}/assets/images/geekshop-kibana-part2.png" />
	<img src="{{site.baseurl}}/assets/images/geekshop-kibana-part3.png" />
</p>







